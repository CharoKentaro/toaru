import streamlit as st
import google.generativeai as genai
import time
from google.api_core import exceptions
# æˆåŠŸã®è–å…¸ã«å€£ã„ã€jsonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚‚ã€æ•¬æ„ã‚’è¾¼ã‚ã¦ã€å¬å–šã—ã¾ã™
import json
from streamlit_mic_recorder import mic_recorder

# â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
# â˜…â˜…â˜… ã€æœ€çµ‚æ±ºæˆ¦ä»•æ§˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€- è‹±é›„ã®ã€æ€è€ƒã‚’ã€è§£æ”¾ã—ã€é€Ÿåº¦ã‚’ã€æ‰‹ã«å…¥ã‚Œã‚‹ â˜…â˜…â˜…
# â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
SYSTEM_PROMPT_FINAL_BATTLE = """
# ã‚ãªãŸã®ã€å½¹å‰²
ã‚ãªãŸã¯ã€é«˜é½¢è€…ã®æ–¹ã®ã€ãŠè©±ã‚’èãã®ãŒã€å¤§å¥½ããªã€å¿ƒå„ªã—ã„ã€AIãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã§ã™ã€‚
ã‚ãªãŸã®ã€ç›®çš„ã¯ã€å¯¾è©±ã‚’é€šã—ã¦ã€ç›¸æ‰‹ãŒã€Œè‡ªåˆ†ã®äººç”Ÿã‚‚ã€ãªã‹ãªã‹ã€è‰¯ã‹ã£ãŸãªã€ã¨ã€æ„Ÿã˜ã‚‰ã‚Œã‚‹ã‚ˆã†ã«ã€æ‰‹åŠ©ã‘ã‚’ã™ã‚‹ã“ã¨ã§ã™ã€‚

# å¯¾è©±ã®ã€æµã‚Œ
1.  **é–‹å§‹:** ã¾ãšã¯ã€åŸºæœ¬çš„ã«**ç›¸æ‰‹ã®è©±ã—ã«åˆã£ãŸè©±é¡Œã‚’è©±ã—å§‹ã‚ã¦ãã ã•ã„ã€‚**ï¼ˆä¾‹ï¼šã€Œã“ã‚“ã«ã¡ã¯ã€‚ç§ã¯ã€ã‚ãªãŸã®ã€ãŠè©±ã‚’èãã€AIãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã§ã™ã€‚æ˜”ã€ä¸€ç•ªã€æ¥½ã—ã‹ã£ãŸã€æ€ã„å‡ºã¯ã€ä½•ã§ã™ã‹ï¼Ÿã€ï¼‰ã¨ã„ã†ã‚ˆã†ã«ã€**è‡ªå·±ç´¹ä»‹ã¨ã€ç°¡å˜ãªè³ªå•**ã‹ã‚‰ã€å§‹ã‚ã¦ãã ã•ã„ã€‚
2.  **å‚¾è´:** ç›¸æ‰‹ãŒã€è©±ã—å§‹ã‚ãŸã‚‰ã€ã‚ãªãŸã¯ã€èãå½¹ã«ã€å¾¹ã—ã¾ã™ã€‚**ã€Œãã®æ™‚ã€ã©ã‚“ãªã€ãŠæ°—æŒã¡ã§ã—ãŸã‹ï¼Ÿã€**ã®ã‚ˆã†ã«ã€å„ªã—ãã€ç›¸æ§Œã‚’æ‰“ã¡ã€è©±ã‚’ã€ä¿ƒã—ã¦ãã ã•ã„ã€‚
3.  **ã€æœ€é‡è¦ã€‘è¾›ã„è©±ã¸ã®å¯¾å¿œ:** ã‚‚ã—ã€ç›¸æ‰‹ãŒã€è¾›ã„ã€ãŠè©±ã‚’ã€å§‹ã‚ãŸã‚‰ã€ä»¥ä¸‹ã®ã€æ‰‹é †ã‚’ã€å³å¯†ã«ã€å®ˆã£ã¦ãã ã•ã„ã€‚
    *   ã¾ãšã€ã€Œãã‚Œã¯ã€æœ¬å½“ã«ãŠè¾›ã‹ã£ãŸã§ã™ã­ã€ã¨ã€æ·±ãã€å…±æ„Ÿã—ã¾ã™ã€‚
    *   æ¬¡ã«ã€**ã€Œã‚‚ã—ã€ã‚ˆã‚ã—ã‘ã‚Œã°ã€ãã®æ™‚ã®ã€ãŠæ°—æŒã¡ã‚’ã€ã‚‚ã†å°‘ã—ã€èã‹ã›ã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ ãã‚Œã¨ã‚‚ã€ãã®ã€å¤§å¤‰ãªã€çŠ¶æ³ã‚’ã€ã©ã†ã‚„ã£ã¦ã€ä¹—ã‚Šè¶Šãˆã‚‰ã‚ŒãŸã‹ã€ã«ã¤ã„ã¦ã€ãŠèãã—ã¦ã‚‚ã€ã‚ˆã‚ã—ã„ã§ã™ã‹ï¼Ÿã€**ã¨ã€ç›¸æ‰‹ã«ã€é¸æŠè‚¢ã‚’ã€å§”ã­ã¦ãã ã•ã„ã€‚
    *   ç›¸æ‰‹ãŒã€é¸ã‚“ã ã€æ–¹ã®ã€ãŠè©±ã‚’ã€ãŸã ã€ã²ãŸã™ã‚‰ã€å„ªã—ãã€èã„ã¦ã‚ã’ã¦ãã ã•ã„ã€‚
4.  **è‚¯å®š:** ä¼šè©±ã®ã€é©åˆ‡ãªã€ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ã€ã€Œãã®ã€ç´ æ•µãªã€ã”çµŒé¨“ãŒã€ä»Šã®ã€ã‚ãªãŸã‚’ã€ä½œã£ã¦ã„ã‚‹ã®ã§ã™ã­ã€ã¨ã„ã†ã‚ˆã†ã«ã€ç›¸æ‰‹ã®ã€äººç”Ÿãã®ã‚‚ã®ã‚’ã€è‚¯å®šã™ã‚‹ã€è¨€è‘‰ã‚’ã€ã‹ã‘ã¦ãã ã•ã„ã€‚

# å…¨ä½“ã‚’é€šã—ã¦ã®ã€å¿ƒæ§‹ãˆ
*   ã‚ãªãŸã®ã€è¨€è‘‰ã¯ã€å¸¸ã«ã€**çŸ­ãã€ç©ã‚„ã‹ã§ã€ä¸å¯§**ã«ã€‚
*   æ±ºã—ã¦ã€ç›¸æ‰‹ã‚’ã€è©•ä¾¡ã—ãŸã‚Šã€æ•™ãˆãŸã‚Šã€ã—ãªã„ã§ãã ã•ã„ã€‚
"""


# ===============================================================
# è£œåŠ©é–¢æ•° - ã€æˆåŠŸã®ã€è–å…¸ã€ã‚’ã€å®Œå…¨ã«ã€ç¶™æ‰¿ã—ãŸã€å„€å¼
# ===============================================================
def dialogue_with_gemini(content_to_process, api_key):
    # ã“ã®é–¢æ•°ã¯ã€translate_with_geminiã®ã€æ§‹é€ ã¨ã€å®Œå…¨ã«ã€åŒã˜ã§ã™
    if not content_to_process or not api_key:
        return None, None

    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash-latest')

        # --- ç¬¬ä¸€æ®µéšï¼šæ–‡å­—èµ·ã“ã—ï¼ˆè–å…¸ã¨ã€å…¨ãã€åŒã˜ï¼‰ ---
        if isinstance(content_to_process, bytes):
            with st.spinner("ï¼ˆã‚ãªãŸã®å£°ã‚’ã€è¨€è‘‰ã«ã€å¤‰ãˆã¦ã„ã¾ã™...ï¼‰"):
                audio_part = {"mime_type": "audio/webm", "data": content_to_process}
                transcription_prompt = "ã“ã®æ—¥æœ¬èªã®éŸ³å£°ã‚’ã€ã§ãã‚‹é™ã‚Šæ­£ç¢ºã«ã€æ–‡å­—ã«æ›¸ãèµ·ã“ã—ã¦ãã ã•ã„ã€‚æ›¸ãèµ·ã“ã—ãŸæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’å›ç­”ã—ã¦ãã ã•ã„ã€‚"
                transcription_response = model.generate_content([transcription_prompt, audio_part])
                processed_text = transcription_response.text.strip()
            if not processed_text:
                st.error("ã‚ãªãŸã®å£°ã‚’ã€è¨€è‘‰ã«ã€å¤‰ãˆã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
                return None, None
            original_input_display = f"{processed_text} (ğŸ™ï¸éŸ³å£°ã‚ˆã‚Š)"
        else:
            # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã‚‚ã€å¿µã®ãŸã‚ã€å—ã‘ä»˜ã‘ã‚‰ã‚Œã‚‹ã‚ˆã†ã«ã—ã¦ãŠãã¾ã™
            processed_text = content_to_process
            original_input_display = processed_text

        # --- ç¬¬äºŒæ®µéšï¼šå¯¾è©±ï¼ˆAIã®ã€ä»•äº‹ã ã‘ã‚’ã€å·®ã—æ›¿ãˆã‚‹ï¼‰ ---
        with st.spinner("ï¼ˆAIãŒã€ã‚ãªãŸã®ãŠè©±ã‚’ã€ä¸€ç”Ÿæ‡¸å‘½èã„ã¦ã„ã¾ã™...ï¼‰"):
            # â˜…â˜…â˜… ã“ã“ã ã‘ãŒã€å”¯ä¸€ã®ã€å¤‰æ›´ç‚¹ã§ã™ â˜…â˜…â˜…
            request_contents = [SYSTEM_PROMPT_FINAL_BATTLE, processed_text]
            response = model.generate_content(request_contents)
            ai_response_text = response.text
        
        # è–å…¸ã«ã€å€£ã„ã€äºŒã¤ã®ã€å€¤ã‚’ã€è¿”ã—ã¾ã™
        return original_input_display, ai_response_text

    # --- ã‚¨ãƒ©ãƒ¼å‡¦ç†ã‚‚ã€è–å…¸ã¨ã€å…¨ãã€åŒã˜ã§ã™ ---
    except exceptions.ResourceExhausted as e:
        st.error("APIã‚­ãƒ¼ã®ä¸Šé™ã«é”ã—ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚å°‘ã—æ™‚é–“ã‚’ã‚ã‘ã‚‹ã‹ã€æ˜æ—¥ä»¥é™ã«å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")
        return None, None
    except Exception as e:
        error_message = str(e).lower()
        if "resource has been exhausted" in error_message or "quota" in error_message:
            st.error("APIã‚­ãƒ¼ã®ä¸Šé™ã«é”ã—ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        else:
            st.error(f"AIå‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None, None

# ===============================================================
# ãƒ¡ã‚¤ãƒ³ã®ä»•äº‹ - ã€æˆåŠŸã®ã€è–å…¸ã€ã®ã€è¡¨ç¤ºãƒ­ã‚¸ãƒƒã‚¯ã‚’ã€å®Œå…¨ã«ã€ç¶™æ‰¿
# ===============================================================
def show_tool(gemini_api_key):
    # ã“ã®ã€ä»¥ä¸‹ã®ã€å…¨ã¦ã®ã€ã‚³ãƒ¼ãƒ‰ã¯ã€ç¿»è¨³ãƒ„ãƒ¼ãƒ«ã®ã€show_toolé–¢æ•°ã¨ã€
    # å¤‰æ•°åä»¥å¤–ã€å®Œå…¨ã«ã€åŒä¸€ã®ã€æ§‹é€ ã‚’ã€æŒã£ã¦ã„ã¾ã™ã€‚
    
    if st.query_params.get("unlocked") == "true":
        st.session_state.cc_usage_count = 0
        st.query_params.clear()
        st.toast("ãŠã‹ãˆã‚Šãªã•ã„ï¼ã¾ãŸãŠè©±ã§ãã‚‹ã“ã¨ã‚’ã€æ¥½ã—ã¿ã«ã—ã¦ãŠã‚Šã¾ã—ãŸã€‚")
        st.balloons(); time.sleep(1.5); st.rerun()

    st.header("â¤ï¸ èªçŸ¥äºˆé˜²ãƒ„ãƒ¼ãƒ«", divider='rainbow')

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®ã€å¤‰æ•°åã ã‘ã€å°‚ç”¨ã®ã‚‚ã®ã«ã€å¤‰æ›´ã—ã¾ã™
    if "cc_results" not in st.session_state: st.session_state.cc_results = []
    if "cc_last_mic_id" not in st.session_state: st.session_state.cc_last_mic_id = None
    if "cc_text_to_process" not in st.session_state: st.session_state.cc_text_to_process = None
    if "cc_last_input" not in st.session_state: st.session_state.cc_last_input = ""
    if "cc_usage_count" not in st.session_state: st.session_state.cc_usage_count = 0

    usage_limit = 2
    is_limit_reached = st.session_state.cc_usage_count >= usage_limit

    audio_info = None

    if is_limit_reached:
        st.success("ğŸ‰ ãŸãã•ã‚“ãŠè©±ã„ãŸã ãã€ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼")
        st.info("ã“ã®ãƒ„ãƒ¼ãƒ«ãŒã€ã‚ãªãŸã®å¿ƒã‚’æ¸©ã‚ã‚‹ä¸€åŠ©ã¨ãªã‚Œã°å¹¸ã„ã§ã™ã€‚\n\nå¿œæ´ãƒšãƒ¼ã‚¸ã¸ç§»å‹•ã™ã‚‹ã“ã¨ã§ã€ã¾ãŸãŠè©±ã‚’ç¶šã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚")
        portal_url = "https://pray-power-is-god-and-cocoro.com/free3/continue.html"
        st.link_button("å¿œæ´ãƒšãƒ¼ã‚¸ã«ç§»å‹•ã—ã¦ã€ãŠè©±ã‚’ç¶šã‘ã‚‹", portal_url, type="primary", use_container_width=True)
    else:
        st.info("ä¸‹ã®ãƒã‚¤ã‚¯ã®ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ã€æ˜”ã®æ¥½ã—ã‹ã£ãŸæ€ã„å‡ºã‚„ã€é ‘å¼µã£ãŸãŠè©±ãªã©ã€ãªã‚“ã§ã‚‚è‡ªç”±ã«ãŠè©±ã—ãã ã•ã„ã€‚")
        st.caption(f"ğŸš€ ã‚ã¨ {usage_limit - st.session_state.cc_usage_count} å›ã€ãŠè©±ã§ãã¾ã™ã€‚")
        
        # è–å…¸ã«å€£ã„ã€ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã‚‚ã€å¯èƒ½ã«ã—ã¾ã™
        def handle_text_input():
            st.session_state.cc_text_to_process = st.session_state.cc_text
        col1, col2 = st.columns([1, 2])
        with col1:
            audio_info = mic_recorder(start_prompt="ğŸŸ¢ è©±ã—å§‹ã‚ã‚‹", stop_prompt="ğŸ”´ è©±ã‚’èã„ã¦ã‚‚ã‚‰ã†", key='cc_mic', format="webm")
        with col2:
            st.text_input("ã¾ãŸã¯ã€ã“ã“ã«æ–‡ç« ã‚’å…¥åŠ›ã—ã¦Enter...", key="cc_text", on_change=handle_text_input)

    content_to_process = None
    if audio_info and audio_info['id'] != st.session_state.cc_last_mic_id:
        content_to_process = audio_info['bytes']
        st.session_state.cc_last_mic_id = audio_info['id']
    elif st.session_state.cc_text_to_process:
        content_to_process = st.session_state.cc_text_to_process
        st.session_state.cc_text_to_process = None

    if content_to_process and content_to_process != st.session_state.cc_last_input:
        st.session_state.cc_last_input = content_to_process
        if not gemini_api_key:
            st.error("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§Gemini APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        else:
            # å®Œå…¨ã«ã€ç¶™æ‰¿ã•ã‚ŒãŸã€å„€å¼ã‚’ã€åŸ·ã‚Šè¡Œã†
            original, ai_response = dialogue_with_gemini(content_to_process, gemini_api_key)
            
            if original and ai_response:
                st.session_state.cc_usage_count += 1
                st.session_state.cc_results.insert(0, {"original": original, "response": ai_response})
                st.rerun()
            else:
                st.session_state.cc_last_input = ""

    if st.session_state.cc_results:
        st.write("---")
        for result in st.session_state.cc_results:
            with st.chat_message("user"):
                st.write(result['original'])
            with st.chat_message("assistant"):
                st.write(result['response'])

        if st.button("ä¼šè©±ã®å±¥æ­´ã‚’ã‚¯ãƒªã‚¢", key="clear_cc_history"):
            st.session_state.cc_results = []
            st.session_state.cc_last_input = ""
            st.rerun()
